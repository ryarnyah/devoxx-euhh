<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<link rel="icon" type="image/png" sizes="32x32" href="favicons/favicon-32x32.png">
		<title>Meuuh</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">

		<style>
			body, .reveal {
				background-image: url(images/slide-background-znk-devfest.png);
				background-size: auto 100%;
				background-repeat: no-repeat;
			}
			.avatar {
				min-width: 7em;
			}
			.reveal .header-tf {
				position: absolute;
				top: 1em;
				right: 1em;
				font-size: 0.5em;
			}
			.reveal .small {
				font-size: 0.8em;
			}
			.reveal .img-rounded {
  				border-radius: 50%;
			}
			.reveal .aligncenter {
				margin: 10px auto 20px;
    			display: block;
			}

			.reveal table th, .reveal table td {
				border-bottom: none;
			}
			.grip-container {
				display: grid;
				grid-template-columns: repeat(3, 1fr);
				grid-auto-rows: minmax(100px, auto);
			}
			.one {
				grid-column: 1 / 3;
				grid-row: 1;
			}
			.two {
				grid-column: 3 / 4;
				grid-row: 1;
			}
			.three {
				grid-column: 3 ;
				grid-row: 2 ;
			}
			.four {
				grid-column: 2;
				grid-row: 3;
			}
     .hide {
         display: none;
         visibility: hidden;
     }
     .show {
         display: block;
         visibility: visible;
     }
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="header-tf">
				<div id="label-container" style="text-align:center"></div>
			</div>
			<div class="slides">
				<section>
					<h1>Euuhh</h1>
					</br>
					<p class="small">
					Code et pr√©sentation </br> https://github.com/louiznk/devfest-nantes-euuhh (ou https://tinyurl.com/devfesteuhh)
					</p>
				</section>
				<section>
					<h3>Qui sommes nous</h3>
					<table class="small">
						<tbody>
							<tr>
								<td><img src="images/pierre_znk.jpg" class="img-rounded aligncenter avatar" /></td>
								<td><img src="images/louis_znk.jpg" class="img-rounded aligncenter avatar"/></td>
							</tr>
							<tr>
								<td>Pierre Morvan <br>@ Zenika Paris</td>
								<td>Louis Tournayre <br>@ Zenika Lyon</td>
							</tr>
						</tbody>
					</table>
					<br>
					üî• nos domaines c‚Äôest le dev Back, le ‚ÄúDevOps‚Äù, pas l‚ÄôIA‚Ä¶. (ni le dev Front) üî•
          <aside class="notes">
              #blabla<br />
              pour ceux qui nous connaissent, nos dommaines c'est le dev Back, le "DevOps", pas l'IA (ni le dev Front).
          </aside>
				</section>
				<section>
			    <h3>Et on a pas vocation de faire ...</h3>
					<img src="images/tesla.gif"/>
          <aside class="notes">
              <b>LOUIS</b><br />
              Mais alors pourquoi un talk √† propos d'IA?<br />
              Et bien il y a quelques mois, Pierre et moi parlions des discours et des tics de langage, et du fait que n'ayant plus personne en pr√©sentiel nous manquions de feedback.<br />
              A d√©faut de trouver une r√©ponse √† votre absence, nous nous sommes dit: "Et si nous utilisions un syst√®me de reconnaissance vocale pour nous signaler nos tics de langage ?"<br />
              <b>PIERRE</b><br />
              Vous imaginez bien que nous avons saut√© sur l'id√©e pour en faire un abstrat, s'amuser, apprendre, prototyper et maintenant partager.<br />
              C'est pour cela que nous allons vous parler de r√©seaux de neurones, de la "repr√©sentation" du son sous forme d'image et que nous aurons un (ou plusieurs) tics du langage qui viendra jouer les troubles-f√™tes dans notre pr√©sentation.<br />
              Comme c'est plus YOLO, nous utiliserons cette pr√©sentation en guise de d√©mo. Nous allons commenc√© par vous pr√©senter les outils que nous avons utilis√© pour faire du machine learning.
          </aside>
				</section>
				<!-- -->
				<section>
					<h3>Teachable & under the hood => Keras/Tensorflow</h3>
					<table>
						<tbody>
							<tr>
								<td width="55%">
									<video>
										<source data-src="images/euuh.mp4" type="video/mp4" />
									</video>
								</td>
								<td width="45%">
									<img src="images/code-tensor.png"/>
								</td>
							</tr>
						</tbody>
					</table>
          <aside class="notes">
              <b>PIERRE</b><br />
              2 approches, celles du "fain√©ant" et celle du fan d'emacs. De mon c√¥t√© je suis pas trop clic-bouton et emacs est mon ami.<br />
              -> Keras & Tensorflow<br />
              <b>LOUIS</b><br />
              Et moi ma librairie c'est Teachable Machine, c'est tout simple d'usage et cela marche du premier coup. En dessous? C'est encore du TensorFlow.<br />
              Et tensorflow c'est quoi? Un librairie de ML utilisant des r√©seaux de neurones.
          </aside>

				</section>
				<!-- -->
				<section>
				  <h2>Zoom sur les r√©seaux de neurones</h2>
          <aside class="notes">
              <b>LOUIS</b><br />
              Prenons un peu le temps pour expliquer ce qu'est un r√©seau de neurones.
          </aside>
				</section>
				<section>
					<h3>Principes r√©seaux de neurones</h3>
					<img src="images/reconnaissance-banane.png"/>
          <aside class="notes">
              <b>LOUIS</b><br />
              Voici un exemple tr√®s grossier de reconnaissance de certains fruits.<br />
              <br />
              Nous avons diff√©rents composants que appellerons neurones.<br />
              Chaque neurone va √™tre sp√©cialis√©<br />
              Il va recevoir une ou plusieures informations en entr√©e et va renvoyer une autre information ou pas d'information du tout.<br />
              <br />
              Dans notre exemple de la banane, une premi√®re couche de neurone sp√©cialis√© dans la couleur re√ßoit l'image de la banane, seul le neuronne jaune transmet un message aux neuronnes suivants.<br />
              Puis les neuronnes de forme re√ßoivent ce m√™me message avec seulement 2 neurones renvoient un autre message au syst√®me de classification.<br />
              <br />
              Le syst√®me de classification va √† partir des messages re√ßu dire que cet objet de couleur jaune ayant la forme d'un demi-cercle et d'un triangle est une banane.
          </aside>
				</section>
				<section>
					<h3>Principes r√©seaux de neurones</h3>
					<table>
						<tbody>
							<tr>
								<td>
									<img src="images/neurone-biologique.png" />
								</td>
								<td>
									<img src="images/neurone-artificiel.png" />
								</td>
							</tr>
						</tbody>
					</table>
          <aside class="notes">
              <b>PIERRE</b><br />
              Explication neurones bio vs art + fonction activation<br />
              Poids sur les liaisons (entrainement)<br />
              <br />
              (dendrites/)synapse -> axione -> synapse<br />
              X -> Y
          </aside>
				</section>
				<section>
					<h3>L'entra√Ænement & backpropagation</h3>
					<img src="images/training-in-progress.gif"/>
          <aside class="notes">
              <b>PIERRE</b><br />
              1. Pr√©sentation schema<br />
              <p>a. Representation classique d'un r√©seau neuronal</p>
              <p>b. chaque couche (layer) compos√©e de N neurones reli√©s entre eux</p>
              2. Entrainement & Backpropagation<br />
              <br />
              <b>[suite PIERRE] -></b>La voix dans ce mod√®le
          </aside>
				</section>
				<!-- -->
				<section>
					<h2>La voix dans ce mod√®le</h2>
          <aside class="notes">
              <b>PIERRE</b><br />
              Et maintenant nous allons vous pr√©senter la representation de la voix.
          </aside>
				</section>
				<section>
					<h3>Voix, sa repr√©sentation et pourquoi</h3>
						<table>
							<tbody>
								<tr>
									<td>
										<img src="images/voix-01.png"/>
									</td>
									<td>
										<img src="images/voix-02.png" class="fragment"/>
									</td>
									<td>
										<img src="images/voix-03.png" class="fragment"/>
									</td>
									<td>
										<img src="images/voix-04.png" class="fragment"/>
									</td>
								</tr>
							</tbody>
						</table>
            <aside class="notes">
                <b>PIERRE</b><br />
                Repr√©sentation signal<br />
                Spectrogramme (transform√©e de Fourier)<br />
                (Utilise un) Mod√®le existant sur la reconnaissance d'image <-> Transfert learning
            </aside>
				</section>
				<section>
					<h3>Explication du flux de traitement</h3>
					<div class="grip-container">
						<div class="one"><img src="images/flux_01.png"/></div>
						<div class="two"><img src="images/flux_02.png" class="fragment"/></div>
						<div class="three"><img src="images/flux_03.png" class="fragment"/></div>
						<!--
						<div class="four"><img src="images/flux_04.png" width="50%" class="fragment"/></div>
						-->
					</div>
          <aside class="notes">
              <b>LOUIS</b><br />
              Comme l'a expliqu√© Pierre nous allons analyser le son en utilisant sa repr√©sentation sous forme d'image.<br />
              <br />
              <b>(+)</b>Ces images sont transform√©es et arrivent au fil de l'eau. Elles sont ajout√©es √† une sorte de ruban visuel.<br />
              La section de fin de ce ruban est analys√© en permanence pour rechercher nos motifs visuels qui correspondent au "Euhh" (et autres patterns classifi√©s). L'analyse est sur un motif suffisamment large pour prendre un "euhh" long sinon j'aurai des alertes √† chaque fois que je dis cheveu ou pneu ou...<br />
              Cette analyse est donc faite sur une sort de fen√™tre glissante sur notre ruban, avec une fen√™tre de la m√™me taille que le motif de notre mod√®le (m√™me si on peut "tricher")<br />
              <br />
              <b>(+)</b> Et donc si le "motif" "euhh" est d√©tect√© alors je peux d√©clencher mon affichage du Euhh.<br />
              <br />
              [Suite: "et nous allons justement voir comment nous affichons dans nos slides le euhh une fois qu'il a √©t√© rep√©r√©"]
          </aside>
				</section>
				<!-- -->
				<section>
					<h2>Du ‚Äúeuhh‚Äù entendu √† üêÆ, c'est ü§™ time</h2>
          <aside class="notes">
              <b>LOUIS</b><br />
              Et nous allons justement voir comment l'on affiche dans notre (webcam/browser) le euhh une fois qu'il a √©t√© rep√©r√©.
          </aside>
				</section>
				<section>
					<section>
						<h3>Une pr√©sentation avec Tensorflow</h3>
						<ul>
							<li>revealjs pour les slides au format web</li>
							<li>tensorflow.js pour utiliser le mod√®le g√©n√©r√©</li>
							<li>et un peu de html et javascript pour li√© cela</li>
						</ul>
					</section>
					<section>
						<h3>L'int√©gration de Tensorflow.js</h3>
						<pre>
							<code data-line-numbers="35|17-18|2-14|19-30">
								<script type="text/template">
					async function createModel() {
						const recognizer = speechCommands.create(
							"BROWSER_FFT", // fourier transform type, not useful to change
							undefined, // speech commands vocabulary feature, not useful for your models
							checkpointURL,
							metadataURL);
					
						// check that model and metadata are loaded via HTTPS requests.
						await recognizer.ensureModelLoaded();
					
						// ..
					
						return recognizer;
					}
					
					async function init() {
						const recognizer = await createModel();
						const labelContainer = document.getElementById("label-container");
						const classLabels = recognizer.wordLabels(); // get class labels
						recognizer.listen(result => {
					
					        const scores = result.scores; // probability of prediction for each class
					        // render the probability scores per class
					        for (let i = 0; i < classLabels.length; i++) {
					            const label = classLabels[i]
					            const score = scores[i].toFixed(2)
								// modify labelContainer
								//...
							}
						}
						
					}
					
					tf.setBackend('wasm').then(() => init());
													</script>
												</code>
											</pre>
			<aside class="notes">
				<b>LOUIS</b><br />
				L'int√©gration dans revealjs est simple<br />				
				1 - Nous chargeons les libs tensorflow.js avec la partie wasm pour avoir un backend en wasm ou lieu de webgl parceque c'est fun</br>
				2 - Nous lan√ßons le chargement et l'initialization du model qui a √©t√© pr√©par√© avant</br>
				3 - Ce model va analyser la voix et nous fournir un "recognizer" de motif de son</br>
				4 - C'est donc avec ce "recognizer" que nous allons pouvoir d√©tecter les "Euuh" et les "Yolos"</br>
			</aside>
					</section>
					<section>
						<h3>L'ajout du "header" dans revealjs</h3>
						<pre>
							<code data-line-numbers="4-6|10-12|13-16">
								<script type="text/template">
									<head>
										<style>										
											.reveal .header-tf {
												position: absolute; top: 1em; right: 1em;
											}
										</style>
									</head>
									<body>
										<div class="header-tf">
											<div id="label-container" style="text-align:center"/>
										</div>
										<div class="slides">
											<section><h1>Slide 1</h1></section>
											<section><h1>Slide 2</h1></section>
										</div>
									</body>
								</script>
							</code>
						</pre>
			<aside class="notes">
				<b>LOUIS</b><br />
				1 - Enfin nous avons un header en haut de la page<br />
				2, 3 - Que l'on va retrouver √† chaque slides car il est en dehors du div utilis√© pour les slides
			</aside>
					</section>
				</section>
				<section>
					<section>
						<h3>La version pour le remote</h3>
						<table>
							<tbody>
								<tr>
									<td>
										<img src="images/pierre_euhh.gif"/>
									</td>
									<td>
										<img src="images/louis_euhh.gif"/>
									</td>
								</tr>
							</tbody>
						</table>
					</section>
					<section>
						<h3>Comment √ßa marche ?</h3>
						<img src="images/implementation-obs.png" />
            <aside class="notes">
				<b>PIERRE</b><br />
				Il s'agit l√† d'une version que nous avons fait pour le mode "remote". Cette version interagit avec notre webcam. Nous
				avons ici 2 entr√©e √† mixer:<br />
				- Un programme qui √©coute le flux audio et qui avec le mod√®le que nous avons entra√Æn√© d√©tecte si le motif sonore "Euhh"
				est pr√©sent. Si il est pr√©sent il affiche Euhh.<br />
				- Et notre webcat qui renvoi notre flux vid√©o<br />
				<br />
				Nous utilisons ensuite un autre programme pour assembler ces 2 flux:<br />
				- Capturer le visuel du programme qui affiche Euhh<br />
				- Le superposer √† notre flux vid√©o mais en ne prenant que le modif Euhh (filtre chromatique)<br />
				- Et transformer tout cela en une sortie vid√©o sous la forme d'une cam√©ra virtuelle qui peut √™tre utilis√©e en tant que
				webcam.
				<br />
                (P5 JS > ML5 JS > TensorFlow)<br />
                (filtre chroma key ou incrustation par chrominance dans OBS)
            </aside>
					</section>
				</section>
				<section>
					<h3>Durant de cette pr√©sentation</h3>
					<div id="score-container-final"></div>
					<br/>
					Aucun animal et aucun √©moji n'ont √©t√© maltrait√©s üíö
				</section>
				<!-- -->
				<section>
					<h2>üôè ‚ùì</h2>
          <aside class="notes">
              <b>LOUIS</b><br />
              merci √† vous, nous esp√©rons que cela vous a appris des choses et que cela vous a donn√© envie de tester.<br />
              <br />
              <b>PIERRE</b><br />
              En cadeau sachez qu'une fois le principe compris, il est tr√®s facile de faire soit m√™me ce type de personnalisation, √† partir de son mais aussi de vid√©o...<br />
              Piloter par exemple les slides avec la voix...<br />
              Ou plus convivial faire un jeu de ni oui ni non.<br />
              <br />
              <b>Pierre ou Louis</b><br />
              Nous partagerons nos slides avec des liens dont les repos github de ce que nous avons fait.<br />
              Et nous sonnes a votre disposition pour r√©pondre √† vos questions.<br />
              Je sais que c'est la fin de la journ√©e? Mais CVP pensez √† nous faitre un retour via les outils dispo<br />
              Merci √† vous.
          </aside>
				</section>
				<section>
					<h3>R√©f√©rences</h3>
					<ul>
						<li>Exemple classification audio avec keras </br> https://www.tensorflow.org/tutorials/audio/simple_audio</li>
						<li>Repos du code </br> https://github.com/louiznk/devfest-nantes-euuhh </li>
						<li>Pr√©sentation online </br> https://louiznk.github.io/devfest-nantes-euuhh/ </li>
						<li>Backpropagation </br> https://www.youtube.com/watch?v=Ilg3gGewQ5U</li>
						<li>TeachableMachine </br> https://teachablemachine.withgoogle.com/</li>
						<li>OBS / MachineLearning </br> https://thecodingtrain.com/CodingChallenges/157-zoom-annotations.html</li>
					</ul>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/highlight/highlight.js"></script>

		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				autoPlayMedia: true,

				width: 1400,
				height: 800,

				// Factor of the display size that should remain empty around
				// the content
				margin: 0.04,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				
				// pdf all fragments in the same page
				pdfSeparateFragments: false,
				pdfMaxPagesPerSlide: 1,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealHighlight, RevealNotes ]
			});
		</script>

		<!-- Import @tensorflow/tfjs or @tensorflow/tfjs-core -->
		<script src="dist/tfjs.js"></script>
		<script src="dist/speech-commands.js"></script>
		
		<!-- Adds the WAsm backend to the global backend registry -->
		<script src="dist/tf-backend-wasm.js"></script>
		
		<script src="js/ml.js"></script>
	</body>
</html>
