<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<link rel="icon" type="image/png" sizes="32x32" href="favicons/favicon-32x32.png">
		<title>Meuuh</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">

		<style>
			body, .reveal {
				background-image: url(images/slide-background-znk-devfest.png);
				background-size: auto 100%;
				background-repeat: no-repeat;
			}
			.avatar {
				  min-width: 7em;
			}
			.reveal .header-tf {
				position: absolute;
				top: 1em;
				right: 1em;
				font-size: 0.5em;
			}
			.reveal .small {
				font-size: 0.8em;
			}
			.reveal .img-rounded {
  				border-radius: 50%;
			}
			.reveal .aligncenter {
				margin: 10px auto 20px;
    			display: block;
			}
			.reveal .center {
				text-align:center;
			}
			.reveal table th, .reveal table td {
				border-bottom: none;
			}
			.grip-container {
				display: grid;
				grid-template-columns: repeat(3, 1fr);
				grid-auto-rows: minmax(100px, auto);
			}
			.one {
				grid-column: 1 / 3;
				grid-row: 1;
			}
			.two {
				grid-column: 3 / 4;
				grid-row: 1;
			}
			.three {
				grid-column: 3 ;
				grid-row: 2 ;
			}
			.four {
				grid-column: 2;
				grid-row: 3;
			}
			.hide {
				display: none;
				visibility: hidden;
			}
			.show {
				display: block;
				visibility: visible;
			}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="header-tf">
				<div id="label-container" style="text-align:center"></div>
			</div>
			<div class="slides">
				<section data-timing="20">
					<h1>Euuhh</h1>
					</br>
					<p class="small">
					Code et pr√©sentation </br> https://github.com/louiznk/devfest-nantes-euuhh (ou https://tinyurl.com/devfesteuhh)
					</p>
					<aside class="notes">
						<b>LOUIS & PIERRE</b><br />
					</aside>
				</section>
				<section data-timing="30">
					<h3>Qui sommes nous</h3>
					<table class="small">
						<tbody>
							<tr>
								<td><img src="images/pierre_znk.png" class="img-rounded aligncenter avatar" /></td>
								<td><img src="images/louis_znk.jpg" class="img-rounded aligncenter avatar"/></td>
							</tr>
							<tr>
								<td class="center">Pierre Morvan @ryarnyah<br>üè¢ Zenika Paris</td>
								<td class="center">Louis Tournayre @_louidji<br>üè¢ Zenika Lyon</td>
							</tr>
						</tbody>
					</table>
					<br>
					üî• nos domaines c‚Äôest le dev Back, le ‚ÄúDevOps‚Äù, pas l‚ÄôIA‚Ä¶. (ni le dev Front) üî•
					<aside class="notes">
						<b>PIERRE & LOUIS</b><br />
						#blabla<br />
						pour ceux qui nous connaissent, nos dommaines c'est le dev Back, le "DevOps", pas l'IA (ni le dev Front).
					</aside>
				</section>
				<section data-timing="60">
				<h3>Et on a pas vocation de faire ...</h3>
					<img src="images/tesla.gif"/>
					<aside class="notes">
						<b>LOUIS</b><br />
						Mais alors pourquoi un talk √† propos d'IA?<br />
						Et bien il y a quelques mois, Pierre et moi parlions des discours et des tics de langage, et du fait que n'ayant plus personne en pr√©sentiel nous manquions de feedback.<br />
						A d√©faut de trouver une r√©ponse √† votre absence, nous nous sommes dit: "Et si nous utilisions un syst√®me de reconnaissance vocale pour nous signaler nos tics de langage ?"<br />
						<b>PIERRE</b><br />
						Vous imaginez bien que nous avons saut√© sur l'id√©e pour en faire un abstract, s'amuser, apprendre, prototyper et maintenant partager.<br />
						C'est pour cela que nous allons vous parler de r√©seaux de neurones, de la "repr√©sentation" du son sous forme d'images et que nous aurons un (ou plusieurs) tics du langage qui viendra jouer les troubles-f√™tes dans notre pr√©sentation.<br />
						Comme c'est plus YOLO, nous utiliserons cette pr√©sentation en guise de d√©mo. Nous allons commenc√© par vous pr√©senter les outils que nous avons utilis√© pour faire du machine learning.
					</aside>
				</section>
				<!-- -->
				<section data-timing="60">
					<h3>Teachable & under the hood => Keras/Tensorflow</h3>
					<table>
						<tbody>
							<tr>
								<td width="55%">
									<video>
										<source data-src="images/euuh.mp4" type="video/mp4" />
									</video>
								</td>
								<td width="45%">
									<img src="images/code-tensor.png"/>
								</td>
							</tr>
						</tbody>
					</table>
					<aside class="notes">
						<b>PIERRE</b><br />
						2 approches, celles du "fain√©ant" et celle du fan d'emacs. De mon c√¥t√© je suis pas trop clic-bouton et emacs est mon ami.<br />
						-> Keras & Tensorflow<br />
						<b>LOUIS</b><br />
						Et moi mon √©diteur c'est Teachable Machine, c'est tout simple d'usage et cela marche du premier coup. En dessous? C'est encore du TensorFlow.<br />
						Et tensorflow c'est quoi? Un librairie de ML utilisant des r√©seaux de neurones.
					</aside>
				</section>
				<!-- -->
				<section data-timing="10">
					<h2>Zoom sur les r√©seaux de neurones</h2>
					<aside class="notes">
						<b>LOUIS</b><br />
						Prenons un peu le temps pour expliquer ce qu'est un r√©seau de neurones.
					</aside>
				</section>
				<section data-timing="120">
					<h3>Principes r√©seaux de neurones</h3>
					<img src="images/reconnaissance-banane.png"/>
					<aside class="notes">
						<b>LOUIS</b><br />
						Voici un exemple tr√®s grossier de reconnaissance de certains fruits.<br />
						<br />
						Nous avons diff√©rents composants que appellerons neurones.<br />
						Chaque neurone va √™tre sp√©cialis√©<br />
						Il va recevoir une ou plusieures informations en entr√©e et va renvoyer une autre information ou pas d'information du tout.<br />
						<br />
						Dans notre exemple de la banane, une premi√®re couche (ou layer) de neurone est sp√©cialis√©e dans la couleur re√ßoit l'image de la banane, seul le neurone jaune transmet un message aux neurones suivants.<br />
						Puis les neurones de forme re√ßoivent ce m√™me message et seulement 2 neurones renvoient un autre message au syst√®me de classification.<br />
						<br />
						Le syst√®me de classification va √† partir des messages re√ßus dire que cet objet de couleur jaune ayant la forme d'un demi-cercle et d'un triangle est une banane.
					</aside>
				</section>
				<section data-timing="120">
					<h3>Principes r√©seaux de neurones</h3>
					<table>
						<tbody>
							<tr>
								<td>
									<img src="images/neurone-biologique.png" />
								</td>
								<td>
									<img src="images/neurone-artificiel.png" />
								</td>
							</tr>
						</tbody>
					</table>
					<aside class="notes">
						<b>PIERRE</b><br />
						Explication neurones bio vs art + fonction activation<br />
						Poids sur les liaisons (entrainement)<br />
						<br />
						(dendrites/)synapse -> axione -> synapse<br />
						X -> Y
					</aside>
				</section>
				<section data-timing="120">
					<h3>L'entra√Ænement & backpropagation</h3>
					<img src="images/training-in-progress.gif"/>
					<aside class="notes">
						<b>PIERRE</b><br />
						1. Pr√©sentation schema<br />
						<p>a. Representation classique d'un r√©seau neuronal</p>
						<p>b. chaque couche (layer) compos√©e de N neurones reli√©s entre eux</p>
						2. Entrainement & Backpropagation<br />
						<br />
						<b>[suite PIERRE] -></b>La voix dans ce mod√®le
					</aside>
				</section>
        <section data-timing="60">
            <h3>L'augmentation de donn√©es</h3>
            <img src="images/6-augment.png" style="max-width: 40%" />
            <aside class="notes">
                <b>PIERRE</b><br />
            </aside>
        </section>
				<!-- -->
				<section data-timing="10">
					<h2>La voix dans ce mod√®le</h2>
					<aside class="notes">
						<b>PIERRE</b><br />
						Et maintenant nous allons vous expliquer comment la voix est representer dans notre mod√®le.
					</aside>
				</section>
				<section data-timing="120">
					<h3>Voix, sa repr√©sentation et pourquoi</h3>
					<table>
						<tbody>
							<tr>
								<td>
									<img src="images/voix-01.png"/>
								</td>
								<td>
									<img src="images/voix-02.png" class="fragment"/>
								</td>
								<td>
									<img src="images/voix-03.png" class="fragment"/>
								</td>
								<td>
									<img src="images/voix-04.png" class="fragment"/>
								</td>
							</tr>
						</tbody>
					</table>
					<aside class="notes">
						<b>PIERRE</b><br />
						Repr√©sentation signal<br />
						Spectrogramme (transform√©e de Fourier)<br />
						(Utilise un) Mod√®le existant sur la reconnaissance d'image <-> Transfert learning
					</aside>
				</section>
				<section data-timing="120">
					<h3>Explication du flux de traitement</h3>
					<div class="grip-container">
						<div class="one"><img src="images/flux_01.png"/></div>
						<div class="two"><img src="images/flux_02.png" class="fragment"/></div>
						<div class="three"><img src="images/flux_03.png" class="fragment"/></div>
						<!--
						<div class="four"><img src="images/flux_04.png" width="50%" class="fragment"/></div>
						-->
					</div>
					<aside class="notes">
						<b>LOUIS</b><br />
						Comme l'a expliqu√© Pierre nous allons analyser le son en utilisant sa repr√©sentation sous forme d'image.<br />
						<br />
						<b>(+)</b>Ces images sont transform√©es et arrivent au fil de l'eau. Elles sont ajout√©es √† une sorte de ruban visuel.<br />
						La section de fin de ce ruban est analys√© en permanence pour rechercher nos motifs visuels qui correspondent au "Euhh" (et autres patterns classifi√©s). L'analyse est sur un motif suffisamment large pour prendre un "euhh" long sinon j'aurai des alertes √† chaque fois que je dis cheveu ou pneu ou...<br />
						Cette analyse est donc faite sur une sort de fen√™tre glissante sur notre ruban, avec une fen√™tre de la m√™me taille que le motif de notre mod√®le (m√™me si on peut "tricher")<br />
						<br />
						<b>(+)</b> Et donc si le "motif" "euhh" est d√©tect√© alors je peux d√©clencher mon affichage du Euhh.<br />
						<br />
						[Suite: "et nous allons justement voir comment nous affichons dans nos slides le euhh une fois qu'il a √©t√© rep√©r√©"]
					</aside>
				</section>
				<!-- -->
				<section data-timing="10">
					<h2>Du ‚Äúeuhh‚Äù entendu √† üêÆ, c'est ü§™ time</h2>
					<aside class="notes">
						<b>LOUIS</b><br />
						Et nous allons justement voir comment l'on affiche dans notre (webcam/browser) le euhh une fois qu'il a √©t√© rep√©r√©.
					</aside>
				</section>
				<section>
					<section data-timing="20">
						<h3>Dans cette pr√©sentation</h3>
						<ul>
							<li>revealjs pour les slides au format web</li>
							<li>tensorflow.js pour utiliser le mod√®le g√©n√©r√©</li>
							<li>et un peu de html et javascript pour li√© cela</li>
						</ul>
						<aside class="notes">
							<b>LOUIS</b><br />
						</aside>
					</section>
					<section data-timing="60">
						<h3>L'int√©gration de Tensorflow.js</h3>
						<pre>
							<code data-line-numbers="35|17-18|2-14|19-30">
								<script type="text/template">
					async function createModel() {
						const recognizer = speechCommands.create(
							"BROWSER_FFT", // fourier transform type, not useful to change
							undefined, // speech commands vocabulary feature, not useful for your models
							checkpointURL,
							metadataURL);
					
						// check that model and metadata are loaded via HTTPS requests.
						await recognizer.ensureModelLoaded();
					
						// ..
					
						return recognizer;
					}
					
					async function init() {
						const recognizer = await createModel();
						const labelContainer = document.getElementById("label-container");
						const classLabels = recognizer.wordLabels(); // get class labels
						recognizer.listen(result => {
					
					        const scores = result.scores; // probability of prediction for each class
					        // render the probability scores per class
					        for (let i = 0; i < classLabels.length; i++) {
					            const label = classLabels[i]
					            const score = scores[i].toFixed(2)
								// modify labelContainer
								//...
							}
						}
						
					}
					
					tf.setBackend('wasm').then(() => init());
								</script>
							</code>
						</pre>
						<aside class="notes">
							<b>LOUIS</b><br />
							L'int√©gration dans revealjs est simple<br />				
							1 - Nous chargeons les libs tensorflow.js avec la partie wasm pour avoir un backend en wasm ou lieu de webgl parceque c'est fun</br>
							2 - Nous lan√ßons le chargement et l'initialization du model qui a √©t√© pr√©par√© avant</br>
							3 - Ce model va analyser la voix et nous fournir un "recognizer" de motif de son</br>
							4 - C'est donc avec ce "recognizer" que nous allons pouvoir d√©tecter les "Euuh", les "Yolos" et tous les autres motifs pour lesquels le model a √©t√© entrain√©</br>
						</aside>
					</section>
					<section data-timing="40">
						<h3>Un "header" pour afficher les tics d√©tect√©s</h3>
						<pre>
							<code data-line-numbers="4-6|10-12|13-16">
								<script type="text/template">
									<head>
										<style>										
											.reveal .header-tf {
												position: absolute; top: 1em; right: 1em;
											}
										</style>
									</head>
									<body>
										<div class="header-tf">
											<div id="label-container" style="text-align:center"/>
										</div>
										<div class="slides">
											<section><h1>Slide 1</h1></section>
											<section><h1>Slide 2</h1></section>
										</div>
									</body>
								</script>
							</code>
						</pre>
						<aside class="notes">
							<b>LOUIS</b><br />
							1 - Enfin nous avons un header en haut de la page<br />
							2, 3 - Que l'on va retrouver √† chaque slides car il est en dehors du div utilis√© pour les slides
						</aside>
					</section>
				</section>
				<section>
					<section data-timing="30">
						<h3>La version pour le remote</h3>
						<table>
							<tbody>
								<tr>
									<td>
										<img src="images/pierre_euhh.gif"/>
									</td>
									<td>
										<img src="images/louis_euhh.gif"/>
									</td>
								</tr>
							</tbody>
						</table>
						<aside class="notes">
							<b>PIERRE</b><br />
						</aside>
					</section>
					<section data-timing="60">
						<h3>Comment √ßa marche ?</h3>
						<img src="images/implementation-obs.png" />
						<aside class="notes">
							<b>PIERRE</b><br />
							Il s'agit l√† d'une version que nous avons fait pour le mode "remote". Cette version interagit avec notre webcam. Nous
							avons ici 2 entr√©e √† mixer:<br />
							- Un programme qui √©coute le flux audio et qui avec le mod√®le que nous avons entra√Æn√© d√©tecte si le motif sonore "Euhh"
							est pr√©sent. Si il est pr√©sent il affiche Euhh.<br />
							- Et notre webcat qui renvoi notre flux vid√©o<br />
							<br />
							Nous utilisons ensuite un autre programme pour assembler ces 2 flux:<br />
							- Capturer le visuel du programme qui affiche Euhh<br />
							- Le superposer √† notre flux vid√©o mais en ne prenant que le modif Euhh (filtre chromatique)<br />
							- Et transformer tout cela en une sortie vid√©o sous la forme d'une cam√©ra virtuelle qui peut √™tre utilis√©e en tant que
							webcam.
							<br />
							(P5 JS > ML5 JS > TensorFlow)<br />
							(filtre chroma key ou incrustation par chrominance dans OBS)
						</aside>
					</section>
				</section>
				<section data-timing="20">
					<h3>Durant de cette pr√©sentation</h3>
					<div id="score-container-final"></div>
					<br/>
					Aucun animal et aucun √©moji n'ont √©t√© maltrait√©s üíö
					<aside class="notes">
						<b>PIERRE</b><br />
					</aside>
				</section>
				<!-- -->
				<section data-timing="30">
					<h2>üôè ‚ùì</h2>
					<aside class="notes">
						<b>LOUIS</b><br />
						Merci √† vous, nous esp√©rons que cela vous a appris des choses et que cela vous a donn√© envie de tester.<br />
						<br />
						<b>PIERRE</b><br />
						En cadeau sachez qu'une fois le principe compris, il est tr√®s facile de faire soit m√™me ce type de personnalisation, √† partir de son mais aussi de vid√©o...<br />
						Piloter par exemple les slides avec la voix...<br />
						<br />
						Nos slides et la partie tensorflow sont dans le repos github publique.<br />
						Nous sommes a votre disposition pour r√©pondre √† vos questions.<br />
						Je sais que c'est la fin de la journ√©e ? Mais SVP pensez √† nous faitre un retour via le mur de Feedback<br />
						Merci √† vous.
					</aside>
				</section>
				<section>
					<h3>R√©f√©rences</h3>
					<ul>
						<li>Exemple classification audio avec keras </br> https://www.tensorflow.org/tutorials/audio/simple_audio</li>
						<li>Repos du code </br> https://github.com/louiznk/devfest-nantes-euuhh </li>
						<li>Pr√©sentation online </br> https://louiznk.github.io/devfest-nantes-euuhh/ </li>
						<li>Backpropagation </br> https://www.youtube.com/watch?v=Ilg3gGewQ5U</li>
						<li>TeachableMachine </br> https://teachablemachine.withgoogle.com/</li>
						<li>OBS / MachineLearning </br> https://thecodingtrain.com/CodingChallenges/157-zoom-annotations.html</li>
					</ul>
					<aside class="notes">
						Questions ? <br />
						Mur de feedback
					</aside>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/highlight/highlight.js"></script>

		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				autoPlayMedia: true,

				width: 1400,
				height: 800,

				// Factor of the display size that should remain empty around
				// the content
				margin: 0.04,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,

				// Slide number on speaker view
				progress: true,
				slideNumber: 'c/t',
				showSlideNumber: 'speaker',

				// 17:30 max sur les slides => 2:00 sur les questions (sur dernier slides) => 19:30
				totalTime: 1170,
				
				// pdf all fragments in the same page
				pdfSeparateFragments: false,
				pdfMaxPagesPerSlide: 1,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealHighlight, RevealNotes ]
			});

			Reveal.configure({
					keyboard: {
						// up key (for remote control)
						38: 'prev',
						// down key (for remote control)
						40: 'next'
					}
				});
		</script>

		<!-- Import @tensorflow/tfjs or @tensorflow/tfjs-core -->
		<script src="dist/tfjs.js"></script>
		<script src="dist/speech-commands.js"></script>
		
		<!-- Adds the WAsm backend to the global backend registry -->
		<script src="dist/tf-backend-wasm.js"></script>
		
		<script src="js/ml.js"></script>
	</body>
</html>
